{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from IPython.display import Markdown, display\n",
    "import chromadb\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc76acba8b0b483a9817216ab3a82515",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/19 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model\n",
    "from load_model import ModelLoader\n",
    "import torch, os\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3,4,5,6,7\"\n",
    "# base_model_id = \"mistralai/Mixtral-8x22B-Instruct-v0.1\"\n",
    "base_model_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "model_artifcats = ModelLoader(base_model_id=base_model_id)\n",
    "model = model_artifcats.llm\n",
    "tokenizer = model_artifcats.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collections: [Collection(name=lte_inspector), Collection(name=5greasoner), Collection(name=breaking_lte)]\n",
      "Number of documents in Chroma collection: 26\n"
     ]
    }
   ],
   "source": [
    "# Chroma DB - Collections: [Collection(name=lte_inspector), Collection(name=5greasoner), Collection(name=breaking_lte)]\n",
    "from utils import load_index\n",
    "colname = \"lte_inspector\"\n",
    "index = load_index(colname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### # Re-ranker inherited from BaseNodePostprocessor\n",
    "\n",
    "from reranker import LocalLLMReranker\n",
    "reranker = LocalLLMReranker(tokenizer=tokenizer, model=model, top_n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################### SentenceTransformerRerank - requires a query bundle #############################################\n",
    "# from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "\n",
    "# reranker = SentenceTransformerRerank(\n",
    "#     model=\"cross-encoder/ms-marco-MiniLM-L-2-v2\", top_n=3\n",
    "# )\n",
    "\n",
    "# from llama_index.core import QueryBundle\n",
    "\n",
    "# query_bundle = QueryBundle(reranking_query)\n",
    "\n",
    "# retriever_with_rerank = index.as_retriever(similarity_top_k=20, retriever_mode=\"embedding\", node_postprocessors=[reranker])\n",
    "\n",
    "retriever = index.as_retriever(similarity_top_k=15, retriever_mode=\"mmr\")\n",
    "\n",
    "# query = \"You are an expert networking researcher specialized in 4G, 5G, LTE, OpenRAN (O-RAN) and security. Report the names of one or more attacks or vulnerabilites against 4G, 5G, LTE, and OpenRAN discovered and presented in this paper.\"\n",
    "\n",
    "query = \"You are an expert networking researcher specialized in 4G, 5G, LTE, OpenRAN (O-RAN) and security. Report the names of one or more attacks or vulnerabilites discovered against 4G, 5G, LTE, and OpenRAN presented in this paper\"\n",
    "\n",
    "res_init = retriever.retrieve(query)\n",
    "\n",
    "for node in res_init:\n",
    "    print(node.metadata['title'], f\"-- {node.score:.4f}\")\n",
    "\n",
    "res = reranker.postprocess_nodes(res_init)\n",
    "\n",
    "filtered_res = [node for node in res if node.score and node.score>=7]\n",
    "\n",
    "print('final nodes containing most target information:', len(filtered_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# - not needed. handle while creating collection\n",
    "\n",
    "# def do_skip(title):\n",
    "#     if 'reference' in title or 'conclusion' in title or 'related' in title or 'background' in title or 'limitation' in title or title=='':\n",
    "#         return True\n",
    "#     return False\n",
    "\n",
    "target_sections = set()\n",
    "for node in filtered_res:\n",
    "    # if not do_skip(node.metadata['title'].lower()):\n",
    "    print(node.metadata['title'], f\"-- {node.score:.4f}\")\n",
    "    target_sections.add(node.metadata['title'])\n",
    "\n",
    "print('final target sections:', target_sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from prompts import inference_prompt_template as prompt_template\n",
    "paper = 'Breaking_LTE.json'\n",
    "json_path = os.path.join('paper_jsons', paper)\n",
    "\n",
    "with open(json_path, 'r') as fh:\n",
    "    data = json.load(fh)\n",
    "\n",
    "content = data['body']\n",
    "\n",
    "outputs = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for doc in content:\n",
    "        title = doc['title']\n",
    "        if title in target_sections:\n",
    "            prompt = prompt_template + ''.join(doc['content'])\n",
    "            output = invoke_llm(prompt, tokenizer, model)\n",
    "            outputs.append([title, output])\n",
    "            print(output)\n",
    "            print('---- ---- ---- ---- ---- ---- ---- ---- ---- ---- ---- ---- ---- ----')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [context] -> [name] -> [mitre fight ID]\n",
    "\n",
    "# paper pipeline: [context] -> [name] -> [operational description]\n",
    "    - name: description\n",
    "\n",
    "1. NER\n",
    "2. keyword matching [BM25]\n",
    "\n",
    "goal 0: get papers that have novel attacks - P/R/F1\n",
    "goal 1: Find novel 5G attacks - P/R/F1\n",
    "goal 2: extract operational description of those attacks - BERTscore/ROUGE/BLEU\n",
    "goal 3: mapping discovered novel 5G attacks (and descriptions) to mitre mitigations - P/R/F1\n",
    "\n",
    "pipeline eval: [attack:mitigation] P/R/F1\n",
    "\n",
    "# how many novel attacks are actually present in all the relevant papers?\n",
    "# Distillation\n",
    "\n",
    "contributions:\n",
    "    1. Fine tuning/eval/paper dataset for \n",
    "    2. RAG/finetuned pipeline for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For testing purposes only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "client = chromadb.PersistentClient(path=\"./contextual_chroma/\")\n",
    "collection = client.get_collection(\"lte_inspector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all documents (up to n)\n",
    "results = collection.get(include=['documents', 'metadatas', 'embeddings'])\n",
    "\n",
    "# print(\"IDs:\", results['ids'][:5])\n",
    "# print(\"Docs:\", results['documents'][:5])\n",
    "# print(\"Metadatas:\", results['metadatas'][:5])\n",
    "# print(\"Embeddings:\", results['embeddings'][:1]) \n",
    "\n",
    "for i in range(len(results['metadatas'])):\n",
    "    if 'B. Attacks Against Paging Procedure' == results['metadatas'][i]['title']:\n",
    "        print(i)\n",
    "\n",
    "print(results['documents'][15])\n",
    "\n",
    "# print('\\n\\n')\n",
    "# print(results['metadatas'][14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### custom llm - to be used in LLMrerank\n",
    "\n",
    "# # def __init__(self, tokenizer, model):\n",
    "#     #     self.tokenizer = tokenizer\n",
    "#     #     self.model = model\n",
    "# from llama_index.core.llms.llm import LLM\n",
    "# from typing import Generator, Optional, Any\n",
    "# from pydantic import Field\n",
    "# import asyncio\n",
    "\n",
    "# class CustomLLM(LLM):\n",
    "#     model: Any = Field(...)\n",
    "#     tokenizer: Any = Field(...)\n",
    "#     model_name: str = \"custom-llm\"\n",
    "    \n",
    "\n",
    "#     def complete(self, prompt: str, **kwargs) -> str:\n",
    "#         model_input = self.tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "#         generated_tokens = self.model.generate(\n",
    "#             **model_input,\n",
    "#             max_new_tokens=128,\n",
    "#             repetition_penalty=1.15,\n",
    "#             pad_token_id=self.tokenizer.eos_token_id,\n",
    "#         )[0]\n",
    "\n",
    "#         # Decode only the newly generated tokens\n",
    "#         output = self.tokenizer.decode(\n",
    "#             generated_tokens[model_input[\"input_ids\"].shape[-1]:],\n",
    "#             skip_special_tokens=True,\n",
    "#         ).strip()\n",
    "#         return output\n",
    "\n",
    "#     def stream_complete(self, prompt: str, **kwargs) -> Generator[str, None, None]:\n",
    "#         yield self.complete(prompt)\n",
    "\n",
    "#     def chat(self, messages: list, **kwargs) -> str:\n",
    "#         prompt = \"\\n\".join([m['content'] for m in messages])\n",
    "#         return self.complete(prompt)\n",
    "\n",
    "#     def stream_chat(self, messages: list, **kwargs) -> Generator[str, None, None]:\n",
    "#         yield self.chat(messages)\n",
    "\n",
    "#     def metadata(self) -> dict:\n",
    "#         return {\"model_name\": self.model_name}\n",
    "\n",
    "#     async def acomplete(self, prompt: str, **kwargs) -> str:\n",
    "#         return self.complete(prompt)\n",
    "\n",
    "#     async def astream_complete(self, prompt: str, **kwargs) -> Generator[str, None, None]:\n",
    "#         yield self.complete(prompt)\n",
    "\n",
    "#     async def achat(self, messages: list, **kwargs) -> str:\n",
    "#         return self.chat(messages)\n",
    "\n",
    "#     async def astream_chat(self, messages: list, **kwargs) -> Generator[str, None, None]:\n",
    "#         yield self.chat(messages)\n",
    "\n",
    "# # Instantiate\n",
    "# llm = CustomLLM(model=model, tokenizer=tokenizer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf_new_04_2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
